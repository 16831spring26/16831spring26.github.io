<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Project Proposal</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 40px;
      background-color: #f9f9f9;
      color: #333;
    }
    header {
      text-align: center;
      margin-bottom: 40px;
    }
    h1 {
      font-size: 2em;
      margin-bottom: 10px;
    }
    h2 {
      font-size: 1.2em;
      font-weight: normal;
      margin-bottom: 20px;
    }
    p {
      max-width: 800px;
      margin: 0 auto 40px auto;
      line-height: 1.6;
    }
    .video-container {
      display: flex;
      justify-content: center;
      gap: 40px;
      flex-wrap: wrap;
    }
    figure {
      width: 45%;
      text-align: center;
      margin: 0;
    }
    video {
      width: 100%;
      border: 2px solid #ccc;
      border-radius: 8px;
    }
    figcaption {
      margin-top: 8px;
      font-weight: bold;
    }
    @media (max-width: 768px) {
      figure {
        width: 100%;
        margin-bottom: 30px;
      }
    }
  </style>
</head>
<body>

  <header>
    <h1>Antman: Hierarchical Policy Transfer from
Point-Mass to Quadrupedal Agents</h1>
    <h2>Felipe San Martín Vásquez, Saswat Subhajyoti Mallick, Srinath Ravi</h2>
  </header>

  <section>
    <p>
      This project explores hierarchical policy transfer by training navigation
policies in a simple Point Maze environment and attempting to transfer them to a
complex Ant Maze with a quadrupedal agent. Although the maze layouts are iden-
tical, these environments have very different action spaces. The Point agent uses
2D linear forces while the Ant agent requires 8-dimensional joint torques. We
will implement two modifications to facilitate this transfer: (1) an action space
alignment module that translates Point agent commands to Ant joint torques, and
(2) state space discretization to speed up learning. We will train three types of RL
algorithms. On-policy policy gradient, off-policy Q-learning, and model-based
RL, on the Point Maze and evaluate how well they transfer to the Ant Maze.
This project is inspired by work on hierarchically decoupled imitation for mor-
phological transfer and aims to help us understand how high-level policies can be
separated from low-level control.
    </p>
  </section>

  <section class="video-container">
    <figure>
      <video src="assets/ant_maze_random_agent.mp4" controls autoplay loop muted></video>
      <figcaption>Ant Maze - Random Agent Run</figcaption>
    </figure>
    <figure>
      <video src="assets/point_maze_random_agent.mp4" controls autoplay loop muted></video>
      <figcaption>Point Maze - Random Agent Run</figcaption>
    </figure>
  </section>

</body>
</html>
